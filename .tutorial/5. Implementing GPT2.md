# Implementing GPT2

You may have tried to load the GPT2-large model via the transformers library but our Repl doesn't have the space. Fortunately we can use the free (rate limited) [inference API to call the model directly.](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task)

You can either make a direct request with 'requests' as shown in the documentation or you can import the InferenceAPI
```python
from huggingface_hub.inference_api import InferenceApi
inference = InferenceApi(repo_id="gpt2-large", token=API_Token)
```
This gives us an endpoint we can call for text generation if we pass in an input string!
```python
input_string = "Learn about AI because"
result = inference(input_string)
print(result)
```

Awesome! We've got output. 
Try it a few times, are you satisfied with the results?