# Improving Performance

Machine learning models can often work out of the box but to improve performance you will have to dive deeper.

Large Language Models (LLM) have two distinct stages: **encoding and decoding**. Encoding is the first process where a LLM takes as text input and converts it into a numeric representation. **The encoding stage can be thought of as the learning stage where our model learns the text input and turns it into something it can understand.**

After the text has been encoded, the decoding stage begins. **The decoding stage is when the model uses its learnings + the input and returns its prediction**, in this case ONE word the model thinks should come next. **_Can you believe that all these capabilities come from just predicting the next word?!_**

In order to improve our model's performance we are going to focus on the decoding stage. 

<p align="center">
<img src="https://miro.medium.com/max/1204/1*o5G3ul7aI0qvuua78tmlhA.png">
</p>

Three popular decoding methods are **deterministic, stochastic, and a new development called contrastive search**. [Huggingface recently released a more detailed writeup here](https://huggingface.co/blog/introducing-csearch). 


## Deterministic Methods
Typically the default method and what we have implemented, this method picks the word with the highest probability of appearing given a prefix text. Although this method delivers consistent results, in practice, the generated output text is prone to repetition and can get stuck in loops.

## Stochastic Methods
Unlike deterministic methods, stochastic methods use more randomness when picking the next word. This approach is much more hit or miss and the output can quickly change topics. 

Two widely-used stochastic methods are top-k sampling and top-p sampling. We can use those methods by changing the parameters, you guessed it, ```top_p``` and ```top_k``` as described in the [Inference API docs](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task). 

```python
top_p_params = {"max_length": 128, "top_p": 0.95, "do_sample":True} # top-p (nucleus sampling)
top_k_params = {"max_length": 128,  "top_k": 4, "do_sample":True} # top-k
```

Try each of these in comparison to Deterministic methods, how do they compare?

![](https://i0.wp.com/analyticsindiamag.com/wp-content/uploads/2022/05/image-29.png)