# What is a Transformer and How to Use Them

Most Large Language Models (LLMs) are based on the Transformer architecture.
Transformers were introduced by the Google Brain team in a famous paper from 2017, [Attention is All You Need](https://arxiv.org/abs/1706.03762). 

You can think of Transformers as a new state of the art way for **computers to understand language** (or any sequential information). We won't dive too deep into how they work but there is a lot of quality content explaining these fascinating tools; one example is [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/) a great visual walkthrough.
![transformer depicted](https://i.imgur.com/y2g3r3r.png)

Huggingface is perhaps most famous for its [Transformers library](https://huggingface.co/docs/transformers/index) which makes downloading and training these models extremely accessible. Today we will be using [GPT2-large](https://huggingface.co/gpt2-large). (Model sizes include gpt2, gpt2-medium, gpt2-large, gpt2-xl. For this tutorial, we are using gpt2-large)

**Before going to the next page, pause here and try implementing GPT2 in main.py on your own**, it is not so straightforward as the model is too big for our Repl.